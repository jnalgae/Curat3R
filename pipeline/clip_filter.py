import torch
import clip
from PIL import Image
import os

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

PROMPTS_MAP = {
    0: [
        "An incomplete image that is only partially loaded.",
        "A photo covered by a large solid color block.",
        "A corrupted file with rendering errors.",
        "Glitch art with digital artifacts."
    ],
    1: [
        "A very blurry photo where details are unrecognizable.",
        "Severe pixelation due to low resolution.",
        "Out of focus photography."
    ],
    2: [
        "A photo containing a human being, person, or people.",
        "A human figure in the frame.",
        "A man, woman, or child.",
        "A portrait or full body shot of a person.",
        "A panoramic landscape of nature or city."
    ],
    3: [
        "A transparent glass or water.",
        "A reflective mirror surface.",
        "An image where the object is cut off by the frame."
    ],
    4: [
        "A background with a brick wall, stone fence, or house siding.",
        "Windows, doors, or architectural details behind the object.",
        "Dense bushes, hedges, or trees directly behind the object.",
        "A busy chaotic scene with urban clutter."
    ],
    5: [
        "A product photo isolated on a plain white or solid color background.",
        "A studio shot with a solid smooth wall.",
        "A minimalist photo with a black or dark background.",
        "A high quality 3D render or cartoon character.",
        "A single object on a large open grass lawn.",
        "An object sitting on an empty floor or pavement.",
        "Delicious food photography."
    ]
}

# Labels and reasons exposed to the frontend
RESULTS_INFO = {
    0: ("[ ì‹œ ìŠ¤ í…œ ë°˜ ë ¤ ]", "íŒŒì¼ ë°ì´í„° ì†ìƒ (ë¡œë”© ì¤‘ë‹¨/ê¹¨ì§)", "ðŸ‘‰ ì •ìƒì ì¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤(ì†ìƒ/ì˜¤ë¥˜).", "íŒŒì¼"),
    1: ("[ í™” ì§ˆ ë°˜ ë ¤ ]", "ì‹¬í•œ íë¦¼ ë˜ëŠ” ì €í•´ìƒë„", "ðŸ‘‰ ì‚¬ì§„ì´ ë„ˆë¬´ íë¦½ë‹ˆë‹¤. ì´ˆì ì„ ë§žì¶”ê³  ë°ì€ ê³³ì—ì„œ ë‹¤ì‹œ ì´¬ì˜í•´ì£¼ì„¸ìš”.", "ì‹¬í•œ"),
    2: ("[ ëŒ€ ìƒ ë°˜ ë ¤ ]", "ì‚¬ëžŒ ë˜ëŠ” í’ê²½ (3D ë³€í™˜ ë¶ˆê°€)", "ðŸ‘‰ ì‚¬ëžŒì´ë‚˜ ê´‘í™œí•œ í’ê²½ì€ 3D ë³€í™˜ ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤.", "ì‚¬ëžŒ"),
    3: ("[ ê¸° ìˆ  ë°˜ ë ¤ ]", "íˆ¬ëª…/ë°˜ì‚¬ ìž¬ì§ˆ ë˜ëŠ” ìž˜ë¦¼", "ðŸ‘‰ ê°ì²´ê°€ ìž˜ë ¸ê±°ë‚˜ íˆ¬ëª…/ë°˜ì‚¬ ìž¬ì§ˆìž…ë‹ˆë‹¤. ì˜¨ì „í•œ ë¶ˆíˆ¬ëª… ê°ì²´ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "íˆ¬ëª…/ë°˜ì‚¬"),
    4: ("[ í™˜ ê²½ ë°˜ ë ¤ ]", "ë°°ê²½ì— êµ¬ì¡°ë¬¼ì´ ë§ŽìŒ", "ðŸ‘‰ ê°ì²´ ë’¤ê°€ ë³µìž¡í•©ë‹ˆë‹¤. ë” ë„“ê³  íŠ¸ì¸ ê³µê°„ì´ë‚˜ ê¹”ë”í•œ ë²½ ì•žì—ì„œ ì°ì–´ë³´ì„¸ìš”.", "ë°°ê²½ì—"),
    5: ("[ í•© ê²© ]", "3D ìƒì„± ì§„í–‰ ê°€ëŠ¥", "ðŸ‘‰ ì™„ë²½í•©ë‹ˆë‹¤! ë°°ê²½ê³¼ ê°ì²´ê°€ ëª¨ë‘ í›Œë¥­í•©ë‹ˆë‹¤. 3D ìƒì„±ì„ ì‹œìž‘í•©ë‹ˆë‹¤.", "3D")
}


# Precompute text embeddings
with torch.no_grad():
    encoded_prompts = {}
    for idx, sentences in PROMPTS_MAP.items():
        tokens = clip.tokenize(sentences).to(device)
        features = model.encode_text(tokens)
        features /= features.norm(dim=-1, keepdim=True)
        encoded_prompts[idx] = features.mean(dim=0) / features.mean(dim=0).norm()
    FINAL_TEXT_FEATURES = torch.stack([v for v in encoded_prompts.values()])

def run_clip_filter(image_path):
    try:
        image_pil = Image.open(image_path).convert("RGB")
        image_input = preprocess(image_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            image_features = model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
            similarity = (100.0 * image_features @ FINAL_TEXT_FEATURES.T).softmax(dim=-1)
            probs = similarity.cpu().numpy()[0]

        if probs[2] > 0.20:
            best_idx = 2
        else:
            best_idx = int(probs.argmax())

        verdict, reason, guide, label = RESULTS_INFO[best_idx]
        status = "accept" if best_idx == 5 else "reject"
        return {
            "status": status,
            "reason": reason,
            "guide": guide
        }
    except Exception as e:
        return {"status": "error", "reason": str(e)}


if __name__ == "__main__":
    import sys
    import json
    if len(sys.argv) != 2:
        print(json.dumps({"status": "error", "reason": "Usage: clip_filter.py <image_path>"}))
        sys.exit(1)
    image_path = sys.argv[1]
    result = run_clip_filter(image_path)
    print(json.dumps(result, ensure_ascii=False))